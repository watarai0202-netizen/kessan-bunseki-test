import hashlib
import re
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional, Tuple

import requests
import streamlit as st

from src.tdnet import fetch_tdnet_items
from src.analyzer import analyze_pdf_to_json, ai_is_enabled
from src.storage import init_db, get_cached_analysis, save_analysis, db_path_default
from src.viz import render_analysis

_JST = timezone(timedelta(hours=9))

# ----------------------------
# Helpers
# ----------------------------

_KESSAN_RE = re.compile(
    r"(æ±ºç®—çŸ­ä¿¡|å››åŠæœŸæ±ºç®—|é€šæœŸæ±ºç®—|Financial Results|Earnings|Results)",
    re.IGNORECASE,
)
_BRIEFING_RE = re.compile(
    r"(æ±ºç®—èª¬æ˜|èª¬æ˜è³‡æ–™|presentation|briefing|supplement|è£œè¶³|Fact\s*Book)",
    re.IGNORECASE,
)

def is_kessan(title: str) -> bool:
    return bool(_KESSAN_RE.search(title or ""))

def is_briefing(title: str) -> bool:
    return bool(_BRIEFING_RE.search(title or ""))

def _parse_dt_any(value: Any) -> Optional[datetime]:
    if not value:
        return None
    s = str(value).strip()
    if not s:
        return None

    s_iso = s.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(s_iso)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=_JST)
        return dt.astimezone(timezone.utc)
    except Exception:
        pass

    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
        try:
            dt = datetime.strptime(s, fmt).replace(tzinfo=_JST)
            return dt.astimezone(timezone.utc)
        except Exception:
            continue
    return None


def _extract_tdnet_fields(it: Dict[str, Any]) -> Tuple[str, str, str, Optional[datetime]]:
    title = (it.get("title") or "").strip()
    code = str(it.get("code") or "").strip()
    doc_url = (it.get("doc_url") or "").strip()
    published_at = it.get("published_at")

    if not isinstance(published_at, datetime):
        published_at = _parse_dt_any(published_at)

    raw = it.get("raw") if isinstance(it.get("raw"), dict) else {}
    td = None
    if isinstance(raw.get("Tdnet"), dict):
        td = raw["Tdnet"]
    elif isinstance(raw.get("TDnet"), dict):
        td = raw["TDnet"]
    elif isinstance(raw.get("tdnet"), dict):
        td = raw["tdnet"]
    elif isinstance(raw, dict):
        td = raw

    if isinstance(td, dict):
        if not title:
            title = str(td.get("title") or td.get("Title") or "").strip()
        if not code:
            code = str(td.get("code") or td.get("company_code") or td.get("Code") or "").strip()
        if not doc_url:
            doc_url = str(
                td.get("document_url")
                or td.get("documentUrl")
                or td.get("doc_url")
                or td.get("url")
                or ""
            ).strip()
        if published_at is None:
            published_at = _parse_dt_any(td.get("published_at") or td.get("pubdate") or td.get("date"))

    return title, code, doc_url, published_at


def _code4(code: str) -> str:
    c = (code or "").strip()
    if len(c) == 5 and c.isdigit() and c.endswith("0"):
        return c[:-1]
    if len(c) >= 4 and c[:4].isdigit():
        return c[:4]
    return c


def _is_allowed_pdf_url(url: str) -> bool:
    u = (url or "").strip()
    if not u:
        return False
    u_low = u.lower()
    if "release.tdnet.info" in u_low and u_low.endswith(".pdf"):
        return True
    if "webapi.yanoshin.jp/rd.php?" in u_low and "release.tdnet.info" in u_low and ".pdf" in u_low:
        return True
    return False


def _pdf_size_bytes(url: str, timeout: float = 10.0) -> Optional[int]:
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout)
        if r.status_code >= 400:
            return None
        cl = r.headers.get("Content-Length")
        if not cl:
            return None
        n = int(cl)
        return n if n > 0 else None
    except Exception:
        return None


def _check_pdf_size_or_warn(url: str, max_bytes: int) -> bool:
    if max_bytes <= 0:
        return True
    n = _pdf_size_bytes(url)
    if n is None:
        st.warning("PDFã‚µã‚¤ã‚ºï¼ˆContent-Lengthï¼‰ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸Šé™è¶…ã®å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯è§£æã«å¤±æ•—ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
        return True
    if n > max_bytes:
        st.error(f"PDFãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™ï¼š{n/1024/1024:.1f}MB > {max_bytes/1024/1024:.1f}MB")
        return False
    return True


def _jst_date_key(published_utc: Optional[datetime]) -> str:
    if isinstance(published_utc, datetime):
        try:
            return published_utc.astimezone(_JST).strftime("%Y-%m-%d")
        except Exception:
            return "unknown"
    return "unknown"


def _sort_key_with_unknown_last(date_key: str) -> Tuple[int, str]:
    return (1, "") if date_key == "unknown" else (0, date_key)


def _doc_rank(title: str) -> int:
    # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ä¸¦ã³é †ï¼šçŸ­ä¿¡ â†’ èª¬æ˜è³‡æ–™ â†’ ãã®ä»–
    if is_kessan(title):
        return 0
    if is_briefing(title):
        return 1
    return 2


# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒŠãƒ¼", layout="wide")

# ----------------------------
# Auth
# ----------------------------
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
if not APP_PASSWORD:
    st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆStreamlit Cloud ã® Secrets ã‹ã€ãƒ­ãƒ¼ã‚«ãƒ«ã® .streamlit/secrets.toml ã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
    pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pw and pw == APP_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
    st.stop()

# ----------------------------
# DB init
# ----------------------------
DB_PATH = st.secrets.get("DB_PATH", db_path_default())
init_db(DB_PATH)

# ----------------------------
# Header
# ----------------------------
st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")
max_pdf_bytes = int(st.secrets.get("MAX_PDF_BYTES", 0) or 0)
if max_pdf_bytes > 0:
    st.caption(f"ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚ / PDFä¸Šé™: {max_pdf_bytes/1024/1024:.1f}MB")
else:
    st.caption("ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚")

# ----------------------------
# Screening controls
# ----------------------------
with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    col1, col2, col3, col4 = st.columns([2, 2, 2, 2])

    with col1:
        code_in = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã€ç©ºãªã‚‰ç›´è¿‘å…¨ä½“ï¼‰", value="").strip()
        only_kessan = st.checkbox("æ±ºç®—çŸ­ä¿¡ã ã‘ã«çµã‚‹ï¼ˆ0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«åˆ‡æ›¿ï¼‰", value=True)

    with col2:
        days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 30, 3)
        limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆå¤§ãã„ã»ã©é…ã„ï¼‰", 50, 500, 200)

    with col3:
        only_has_doc_url = st.checkbox("PDF URLãŒã‚ã‚‹ã‚‚ã®ã ã‘", value=False)
        show_ai_button = st.checkbox("AIåˆ†æãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º", value=True)

    with col4:
        show_debug = st.checkbox("DEBUGè¡¨ç¤ºï¼ˆå…ˆé ­5ä»¶ã®JSONï¼‰", value=False)
        show_n_groups = st.slider("ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—æ•°", 10, 200, 60)

# sanity for code
code = ""
if code_in:
    if code_in.isdigit() and len(code_in) == 4:
        code = code_in
    else:
        st.warning("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¯4æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š7203ï¼‰")

# ----------------------------
# Fetch TDnet + cache
# ----------------------------
cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)

@st.cache_data(ttl=60, show_spinner=False)
def _cached_fetch_tdnet_items(code_: Optional[str], limit_: int) -> list[dict[str, Any]]:
    return fetch_tdnet_items(code_, limit=limit_)

with st.spinner("é–‹ç¤ºä¸€è¦§ã‚’å–å¾—ä¸­..."):
    items = _cached_fetch_tdnet_items(code or None, limit)

if show_debug:
    st.subheader("DEBUG: items å…ˆé ­5ä»¶ï¼ˆtitle/code/doc_url/link ã®æºã‚Œç¢ºèªï¼‰")
    st.json(items[:5])

# ----------------------------
# Normalize + Filter
# ----------------------------
normalized: list[dict[str, Any]] = []
for it in items:
    if not isinstance(it, dict):
        continue
    title, code_raw, doc_url, published_at = _extract_tdnet_fields(it)
    code4 = _code4(code_raw)

    normalized.append(
        {
            "title": title,
            "code": code4,
            "code_raw": code_raw,
            "doc_url": doc_url,
            "published_at": published_at,  # UTC
            "raw": it.get("raw") if isinstance(it.get("raw"), dict) else it,
        }
    )

def apply_filters(use_kessan: bool) -> list[dict[str, Any]]:
    out: list[dict[str, Any]] = []
    for it in normalized:
        title = it.get("title", "")
        doc_url = (it.get("doc_url") or "").strip()
        published = it.get("published_at")

        if use_kessan and not is_kessan(title):
            continue
        if only_has_doc_url and not doc_url:
            continue
        if isinstance(published, datetime) and published < cutoff_utc:
            continue
        out.append(it)
    return out

filtered = apply_filters(only_kessan)
if only_kessan and not filtered:
    st.info("ã€æ±ºç®—çŸ­ä¿¡ã ã‘ã€ã§0ä»¶ã ã£ãŸã®ã§ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’åºƒã’ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
    filtered = apply_filters(False)

st.subheader(f"å€™è£œï¼ˆè³‡æ–™æ•°ï¼‰ï¼š{len(filtered)}ä»¶")
if not filtered:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹é–‹ç¤ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥æ•°ã‚„ä»¶æ•°ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# AI availability
ai_ok = ai_is_enabled()
if show_ai_button and not ai_ok:
    st.warning("Gemini APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€AIåˆ†æã¯ç„¡åŠ¹ã§ã™ã€‚Secretsã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# Grouping: (code, date_jst)
# ----------------------------
groups: dict[tuple[str, str], list[dict[str, Any]]] = defaultdict(list)
for it in filtered:
    code4 = it.get("code") or "----"
    date_key = _jst_date_key(it.get("published_at"))
    groups[(code4, date_key)].append(it)

# sort groups: newest date first, unknown last
group_keys = sorted(
    groups.keys(),
    key=lambda k: _sort_key_with_unknown_last(k[1]),
    reverse=True,
)

st.subheader(f"è¡¨ç¤ºã‚°ãƒ«ãƒ¼ãƒ—ï¼š{min(len(group_keys), show_n_groups)} / {len(group_keys)}")

# ----------------------------
# Render groups
# ----------------------------
for gi, gk in enumerate(group_keys[:show_n_groups]):
    code4, date_key = gk
    docs = groups[gk]

    # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ä¸¦ã³
    docs_sorted = sorted(
        docs,
        key=lambda d: (_doc_rank(d.get("title", "")), (d.get("published_at") or datetime(1970, 1, 1, tzinfo=timezone.utc))),
    )

    # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ï¼ˆæ—¥æ™‚ã¯æœ€ã‚‚æ–°ã—ã„ã‚‚ã®ã‚’æ¡ç”¨ï¼‰
    latest_dt = None
    for d in docs_sorted:
        if isinstance(d.get("published_at"), datetime):
            latest_dt = d["published_at"]
            break
    if isinstance(latest_dt, datetime):
        latest_str = latest_dt.astimezone(_JST).strftime("%Y-%m-%d %H:%M JST")
    else:
        latest_str = f"{date_key}ï¼ˆæ—¥æ™‚ä¸æ˜ï¼‰"

    group_label = f"{code4}ï½œ{date_key}ï½œè³‡æ–™{len(docs_sorted)}ä»¶ï¼ˆæœ€çµ‚: {latest_str}ï¼‰"

    with st.expander(group_label, expanded=False):
        # ã¾ãšã¯ã‚°ãƒ«ãƒ¼ãƒ—æ¦‚è¦
        st.caption("åŒä¸€éŠ˜æŸ„ãƒ»åŒæ—¥ã®è³‡æ–™ã‚’ã¾ã¨ã‚ã¦è¡¨ç¤ºï¼ˆçŸ­ä¿¡â†’èª¬æ˜è³‡æ–™â†’ãã®ä»–ã®é †ï¼‰ã€‚")

        for di, it in enumerate(docs_sorted):
            title = it.get("title", "")
            code_raw = it.get("code_raw", "") or ""
            doc_url = (it.get("doc_url") or "").strip()
            published = it.get("published_at")

            if isinstance(published, datetime):
                published_str = published.astimezone(_JST).strftime("%Y-%m-%d %H:%M JST")
            else:
                published_str = "æ—¥æ™‚ä¸æ˜"

            # 1è³‡æ–™ã”ã¨ã®UID
            seed = f"{code4}|{date_key}|{doc_url}|{published_str}|{title}|{gi}|{di}"
            uid = hashlib.md5(seed.encode("utf-8")).hexdigest()[:12]

            # è³‡æ–™ã‚¿ã‚¤ãƒ—ãƒãƒƒã‚¸
            tag = "çŸ­ä¿¡" if is_kessan(title) else ("èª¬æ˜" if is_briefing(title) else "è³‡æ–™")

            st.markdown(f"---\n**[{tag}] {title}**  \n`{published_str}`  \nã‚³ãƒ¼ãƒ‰: {code4}({code_raw})")

            doc_url = (doc_url or "").strip()

           if doc_url.startswith("http"):
           st.markdown(f"[PDFã‚’é–‹ã]({doc_url})")
           st.caption(f"PDF: {doc_url}")
           elif doc_url:
           st.warning("PDF URLãŒä¸æ­£å½¢å¼ã®ãŸã‚ãƒªãƒ³ã‚¯ã‚’å‡ºã›ã¾ã›ã‚“ã€‚")
           st.code(doc_url)
           else:
        st.caption("PDF: ï¼ˆãªã—ï¼‰")


            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¡¨ç¤º
            cached = get_cached_analysis(DB_PATH, doc_url) if doc_url else None
            if cached:
                st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
                render_analysis(cached)
                continue

            st.info("æœªè§£æ")

            allowed = bool(doc_url) and _is_allowed_pdf_url(doc_url)
            can_run_ai = show_ai_button and ai_ok and allowed

            if doc_url and not allowed:
                st.warning("å®‰å…¨ã®ãŸã‚ã€ã“ã®PDF URLã¯AIè§£æå¯¾è±¡å¤–ã§ã™ï¼ˆrelease.tdnet.info ã‚‚ã—ãã¯ yanoshin rd.php çµŒç”±ã®ã¿è¨±å¯ï¼‰ã€‚")

            run = st.button("AIåˆ†æ", key=f"ai_{uid}", disabled=not can_run_ai)

            if run:
                if not _check_pdf_size_or_warn(doc_url, max_pdf_bytes):
                    st.stop()

                with st.spinner("AIãŒæ±ºç®—çŸ­ä¿¡ã‚’è§£æä¸­..."):
                    try:
                        payload = analyze_pdf_to_json(doc_url)
                        save_analysis(DB_PATH, doc_url, code4, title, published, payload)
                        st.success("è§£æå®Œäº†")
                        render_analysis(payload)
                    except Exception as e:
                        st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

st.divider()

# ----------------------------
# Manual analyze
# ----------------------------
st.subheader("æ‰‹å‹•è§£æï¼ˆPDF URLã‚’è²¼ã‚‹ï¼‰")
manual = st.text_input("PDF URLï¼ˆrelease.tdnet.info ã® .pdf æ¨å¥¨ï¼‰", value="").strip()

colA, colB = st.columns([1, 3])
with colA:
    manual_allowed = _is_allowed_pdf_url(manual)
    manual_ok = ai_ok and manual_allowed
    manual_run = st.button("AIè§£æ", disabled=not manual_ok)

with colB:
    if manual and not manual_allowed:
        st.warning("å®‰å…¨ã®ãŸã‚ã€release.tdnet.info ã®PDFï¼ˆã¾ãŸã¯ yanoshin rd.php çµŒç”±ï¼‰ä»¥å¤–ã¯ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã¾ã™ã€‚")
    else:
        st.caption("â€»AIæœ‰åŠ¹ï¼‹è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã®PDF URLã®ã¿è§£æã—ã¾ã™ã€‚")

if manual_run:
    if not _check_pdf_size_or_warn(manual, max_pdf_bytes):
        st.stop()

    cached = get_cached_analysis(DB_PATH, manual)
    if cached:
        st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
        render_analysis(cached)
    else:
        with st.spinner("AIãŒè§£æä¸­..."):
            try:
                payload = analyze_pdf_to_json(manual)
                save_analysis(DB_PATH, manual, "", "manual", None, payload)
                st.success("è§£æå®Œäº†")
                render_analysis(payload)
            except Exception as e:
                st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
