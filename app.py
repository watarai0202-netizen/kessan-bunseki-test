import hashlib
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse

import streamlit as st

from src.tdnet import fetch_tdnet_items
from src.analyzer import analyze_pdf_to_json, ai_is_enabled
from src.storage import init_db, get_cached_analysis, save_analysis, db_path_default
from src.viz import render_analysis

# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒŠãƒ¼", layout="wide")

# ----------------------------
# Auth (simple password gate)
# ----------------------------
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
if not APP_PASSWORD:
    st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆSecretsã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
    pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pw == APP_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
    st.stop()

# ----------------------------
# DB init (cache store)
# ----------------------------
DB_PATH = st.secrets.get("DB_PATH", db_path_default())
init_db(DB_PATH)

# ----------------------------
# Security: strict URL allowlist (TDnet official only)
# ----------------------------
# â€»ã€Œæ‰‹å‹•URLã€ã¯ç‰¹ã«å±é™ºã«ãªã‚Šã‚„ã™ã„ã®ã§ã€å…¬å¼ç³»ã®ã¿è¨±å¯
ALLOWED_HOST_SUFFIXES = (
    "release.tdnet.info",
)

def is_allowed_pdf_url(url: str) -> bool:
    try:
        u = urlparse(url)
        if u.scheme not in ("http", "https"):
            return False
        host = (u.hostname or "").lower()
        if not host:
            return False
        # allow subdomains too (e.g., xxx.release.tdnet.info)
        if not any(host == s or host.endswith("." + s) for s in ALLOWED_HOST_SUFFIXES):
            return False
        # å¼·ã‚ï¼šæ‹¡å¼µå­ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨ã§ã¯ãªã„ãŒäº‹æ•…ã‚’æ¸›ã‚‰ã™ï¼‰
        if not u.path.lower().endswith(".pdf"):
            return False
        return True
    except Exception:
        return False

def short_key(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()[:12]

# ----------------------------
# Header
# ----------------------------
st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")
st.caption("ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€ŒéŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚")
st.caption("â€»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·ã‚ï¼šæ‰‹å‹•URLã¯TDnetå…¬å¼ï¼ˆrelease.tdnet.info ã®PDFï¼‰ã ã‘è¨±å¯ã€‚PDFã‚µã‚¤ã‚ºä¸Šé™ã‚ã‚Šã€‚")

# ----------------------------
# Screening controls
# ----------------------------
with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    col1, col2, col3 = st.columns([2, 2, 2])

    with col1:
        code = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã€ç©ºãªã‚‰ç›´è¿‘å…¨ä½“ï¼‰", value="").strip()
        only_kessan = st.checkbox("æ±ºç®—çŸ­ä¿¡ã ã‘ã«çµã‚‹", value=True)

    with col2:
        days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 14, 3)
        limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆå¤§ãã„ã»ã©é…ã„ï¼‰", 50, 500, 200)

    with col3:
        only_has_doc_url = st.checkbox("PDF URLãŒã‚ã‚‹ã‚‚ã®ã ã‘", value=True)
        show_ai_button = st.checkbox("AIåˆ†æãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º", value=True)

# sanity for code
if code and (not code.isdigit() or len(code) != 4):
    st.warning("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¯4æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š7203ï¼‰")
    code = ""

# ----------------------------
# Fetch TDnet index (non-scrape)
# ----------------------------
cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)
with st.spinner("é–‹ç¤ºä¸€è¦§ã‚’å–å¾—ä¸­..."):
    items = fetch_tdnet_items(code or None, limit=limit)

# Filter
filtered = []
for it in items:
    title = (it.get("title") or "").strip()
    doc_url = (it.get("doc_url") or "").strip()
    published = it.get("published_at")

    if only_kessan and "æ±ºç®—çŸ­ä¿¡" not in title:
        continue
    if only_has_doc_url and not doc_url:
        continue
    if published and published < cutoff_utc:
        continue

    filtered.append(it)

st.subheader(f"å€™è£œï¼š{len(filtered)}ä»¶")
if not filtered:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹é–‹ç¤ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥æ•°ã‚„ä»¶æ•°ã‚’åºƒã’ã¦ãã ã•ã„ã€‚")
    st.stop()

# AI availability
ai_ok = ai_is_enabled()
if show_ai_button and not ai_ok:
    st.warning("Gemini APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€AIåˆ†æã¯ç„¡åŠ¹ã§ã™ï¼ˆæ•°å€¤è¡¨ç¤ºã®ã¿ï¼‰ã€‚Secretsã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# Render list
# ----------------------------
for it in filtered[:100]:
    title = it.get("title", "")
    code_ = it.get("code", "")
    doc_url = it.get("doc_url", "")
    published = it.get("published_at")
    published_str = published.astimezone(timezone.utc).strftime("%Y-%m-%d %H:%M UTC") if published else "ä¸æ˜"

    label = f"{code_}ï½œ{published_str}ï½œ{title}"
    with st.expander(label, expanded=False):
        st.caption(f"PDF: {doc_url}")

        cached = get_cached_analysis(DB_PATH, doc_url) if doc_url else None
        if cached:
            st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
            render_analysis(cached)
        else:
            st.info("æœªè§£æ")

        k = short_key(doc_url or label)

        cols = st.columns([1, 1, 2])
        with cols[0]:
            if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¡¨ç¤º", key=f"show_{k}") and cached:
                render_analysis(cached)

        with cols[1]:
            can_run_ai = show_ai_button and ai_ok and bool(doc_url) and is_allowed_pdf_url(doc_url)
            run = st.button("AIåˆ†æ", key=f"ai_{k}", disabled=not can_run_ai)

            if show_ai_button and ai_ok and doc_url and (not is_allowed_pdf_url(doc_url)):
                st.caption("AIåˆ†æã¯TDnetå…¬å¼PDFï¼ˆrelease.tdnet.info ã® .pdfï¼‰ã ã‘è¨±å¯ã—ã¦ã„ã¾ã™ã€‚")

        with cols[2]:
            st.caption("â€»åŒã˜PDF URLã¯SQLiteã«ä¿å­˜ã—ã€å†è§£æã—ã¾ã›ã‚“ï¼ˆDBã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ‰±ã„ï¼‰ã€‚")

        if run:
            with st.spinner("AIãŒæ±ºç®—çŸ­ä¿¡ã‚’è§£æä¸­..."):
                try:
                    payload = analyze_pdf_to_json(doc_url)
                    save_analysis(DB_PATH, doc_url, code_, title, published, payload)
                    st.success("è§£æå®Œäº†")
                    render_analysis(payload)
                except Exception as e:
                    st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

st.divider()

# ----------------------------
# Manual analyze (STRICT)
# ----------------------------
st.subheader("æ‰‹å‹•è§£æï¼ˆTDnetå…¬å¼PDFã®ã¿ï¼‰")
manual = st.text_input("PDF URLï¼ˆrelease.tdnet.info ã‹ã¤ .pdf ã®ã¿ï¼‰", value="").strip()

colA, colB = st.columns([1, 3])
with colA:
    manual_ok = ai_ok and bool(manual) and is_allowed_pdf_url(manual)
    manual_run = st.button("AIè§£æ", disabled=not manual_ok)

with colB:
    if manual and not is_allowed_pdf_url(manual):
        st.warning("æ‰‹å‹•è§£æã¯ TDnetå…¬å¼ï¼ˆrelease.tdnet.infoï¼‰ã‹ã¤ .pdf ã®URLã®ã¿è¨±å¯ã—ã¦ã„ã¾ã™ã€‚")

if manual_run:
    with st.spinner("AIãŒè§£æä¸­..."):
        payload = analyze_pdf_to_json(manual)
    st.json(payload)
