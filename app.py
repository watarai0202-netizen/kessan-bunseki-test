import hashlib
import re
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional
from urllib.parse import urlparse

import streamlit as st

from src.tdnet import fetch_tdnet_items
from src.analyzer import analyze_pdf_to_json, ai_is_enabled
from src.storage import init_db, get_cached_analysis, save_analysis, db_path_default
from src.viz import render_analysis

# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒŠãƒ¼", layout="wide")

# ----------------------------
# Auth (simple password gate)
# ----------------------------
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
if not APP_PASSWORD:
    st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆStreamlit Cloudã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
    pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pw == APP_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
    st.stop()

# ----------------------------
# DB init (cache store)
# ----------------------------
DB_PATH = st.secrets.get("DB_PATH", db_path_default())
init_db(DB_PATH)

# ----------------------------
# Regex
# ----------------------------
_RE_KESSAN_STRICT = re.compile(r"(æ±ºç®—çŸ­ä¿¡)", re.IGNORECASE)
_RE_KESSAN_WIDE = re.compile(
    r"(æ±ºç®—çŸ­ä¿¡|å››åŠæœŸ|é€šæœŸ|æ±ºç®—èª¬æ˜|Financial Results|Earnings|Results|æ¥­ç¸¾|æ¥­ç¸¾äºˆæƒ³|å£²ä¸Šåç›Š|æœˆæ¬¡)",
    re.IGNORECASE,
)

def is_kessan_strict(title: str) -> bool:
    return bool(_RE_KESSAN_STRICT.search(title or ""))

def is_kessan_wide(title: str) -> bool:
    return bool(_RE_KESSAN_WIDE.search(title or ""))

# ----------------------------
# Helpers (å£Šã‚Œã«ãã•æœ€å„ªå…ˆ)
# ----------------------------
def _parse_dt_any(v: Any) -> Optional[datetime]:
    if not v:
        return None
    s = str(v).strip().replace("Z", "+00:00")

    # ISO
    try:
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        pass

    # "YYYY-MM-DD HH:MM:SS"
    try:
        dt = datetime.strptime(s, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None

def _unwrap_raw(it: Dict[str, Any]) -> Dict[str, Any]:
    """
    it["raw"] ãŒ {"Tdnet": {...}} / {"TDnet": {...}} / {"tdnet": {...}} ã®ã‚ˆã†ã«åŒ…ã¾ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã‚„
    itè‡ªä½“ãŒãã‚Œã«è¿‘ã„ã‚±ãƒ¼ã‚¹ã§ã‚‚ã€ä¸­èº«dictã‚’å–ã‚Šå‡ºã™ã€‚
    """
    raw = it.get("raw")
    if isinstance(raw, dict):
        for k in ("TDnet", "Tdnet", "tdnet"):
            if isinstance(raw.get(k), dict):
                return raw.get(k)
        return raw

    # å¿µã®ãŸã‚ it è‡ªä½“ã‚‚è¦‹ã‚‹
    for k in ("TDnet", "Tdnet", "tdnet"):
        if isinstance(it.get(k), dict):
            return it.get(k)

    return it

def _pick_first(*vals: Any) -> str:
    for v in vals:
        if v is None:
            continue
        if isinstance(v, str):
            vv = v.strip()
            if vv:
                return vv
        else:
            try:
                vv = str(v).strip()
                if vv:
                    return vv
            except Exception:
                pass
    return ""

def normalize_in_app(it: Dict[str, Any]) -> Dict[str, Any]:
    """
    src/tdnet.py ãŒå£Šã‚Œã¦ã‚‚è¡¨ç¤ºãŒæ­»ãªãªã„ã‚ˆã†ã«ã€
    it ã¨ raw ã®ä¸¡æ–¹ã‹ã‚‰å¿…è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿å®ˆçš„ã«å¾©å…ƒã™ã‚‹ã€‚
    """
    td = _unwrap_raw(it)

    title = _pick_first(
        it.get("title"),
        td.get("title"),
        td.get("Title"),
        td.get("subject"),
        td.get("Subject"),
    )

    code = _pick_first(
        it.get("code"),
        td.get("code"),
        td.get("Code"),
        td.get("company_code"),   # â†ã‚¹ã‚¯ã‚·ãƒ§ã§ã“ã‚Œ
        td.get("ticker"),
    )

    doc_url = _pick_first(
        it.get("doc_url"),
        td.get("document_url"),   # â†ã‚¹ã‚¯ã‚·ãƒ§ã§ã“ã‚Œ
        td.get("documentUrl"),
        td.get("doc_url"),
        td.get("pdf_url"),
        td.get("url"),
    ).strip()

    link = _pick_first(
        it.get("link"),
        td.get("link"),
        td.get("url"),
        td.get("detail_url"),
    ).strip()

    published = it.get("published_at")
    if not isinstance(published, datetime):
        published = _parse_dt_any(
            it.get("published_at")
        ) or _parse_dt_any(
            td.get("published_at")
        ) or _parse_dt_any(
            td.get("pubdate")  # â†ã‚¹ã‚¯ã‚·ãƒ§ã§ã“ã‚Œ
        ) or _parse_dt_any(
            td.get("date")
        )

    # è¡¨ç¤ºç”¨ã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼š5æ¡ã®å ´åˆã¯æœ«å°¾4æ¡ã‚’ä½µè¨˜ï¼ˆå¥½ã¿ã§ï¼‰
    code_disp = code
    if code.isdigit() and len(code) == 5:
        code_disp = f"{code[-4:]}({code})"

    # å®‰å…¨ãª uidï¼ˆbutton keyé‡è¤‡ã‚’æ½°ã™ï¼‰
    seed_parts = [
        _pick_first(td.get("id"), it.get("id"), ""),
        code,
        str(published) if published else "",
        title,
        doc_url,
        link,
    ]
    seed = "|".join(seed_parts)
    uid = hashlib.md5(seed.encode("utf-8")).hexdigest()[:12]

    return {
        "title": title,
        "code": code,
        "code_disp": code_disp,
        "doc_url": doc_url,
        "link": link,
        "published_at": published,
        "uid": uid,
        "raw": td,
    }

def is_allowed_final_pdf_host(final_url: str) -> bool:
    host = urlparse(final_url).netloc.lower()
    return host.endswith("release.tdnet.info")

# ----------------------------
# Header
# ----------------------------
st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")
st.caption("ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€ŒéŠ˜æŸ„â†’é–‹ç¤ºâ†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚")
st.caption("â€» PDFä¸Šé™ã¯ Secrets ã® MAX_PDF_BYTES ã§åˆ¶å¾¡ï¼ˆæœªè¨­å®šãªã‚‰ analyzer å´ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")

# ----------------------------
# Screening controls
# ----------------------------
with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    col1, col2, col3 = st.columns([2, 2, 2])

    with col1:
        code_in = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆç©ºãªã‚‰ç›´è¿‘å…¨ä½“ï¼‰", value="").strip()
        only_kessan = st.checkbox("æ±ºç®—çŸ­ä¿¡ã ã‘ã«çµã‚‹ï¼ˆ0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«åˆ‡æ›¿ï¼‰", value=False)

    with col2:
        days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 30, 12)
        limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆå¤§ãã„ã»ã©é…ã„ï¼‰", 50, 800, 300)

    with col3:
        only_has_doc_url = st.checkbox("PDF URLãŒã‚ã‚‹ã‚‚ã®ã ã‘", value=False)
        show_ai_button = st.checkbox("AIåˆ†æãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º", value=True)
        debug_show = st.checkbox("DEBUGè¡¨ç¤ºï¼ˆå…ˆé ­5ä»¶ã®JSONï¼‰", value=False)

# ----------------------------
# Fetch TDnet index
# ----------------------------
cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)
with st.spinner("é–‹ç¤ºä¸€è¦§ã‚’å–å¾—ä¸­..."):
    items_raw = fetch_tdnet_items(code_in or None, limit=limit)

# appå´ã§æœ€çµ‚æ­£è¦åŒ–ï¼ˆä¿é™ºï¼‰
items = [normalize_in_app(it) for it in items_raw]

if debug_show:
    st.subheader("DEBUG: å–å¾—çŠ¶æ³")
    st.write(
        {
            "items_total": len(items),
            "has_title": sum(1 for x in items if x["title"]),
            "has_doc_url": sum(1 for x in items if x["doc_url"]),
            "has_published": sum(1 for x in items if x["published_at"] is not None),
        }
    )
    st.json(items[:5])

# ----------------------------
# Filter builder
# ----------------------------
def build_filtered(use_strict: bool) -> list[dict[str, Any]]:
    out = []
    for it in items:
        title = it.get("title") or ""
        doc_url = (it.get("doc_url") or "").strip()
        published = it.get("published_at")

        if only_kessan:
            ok = is_kessan_strict(title) if use_strict else is_kessan_wide(title)
            if not ok:
                continue

        if only_has_doc_url and not doc_url:
            continue

        if isinstance(published, datetime) and published < cutoff_utc:
            continue

        out.append(it)
    return out

# strict -> fallback to wide if zero
filtered = build_filtered(use_strict=True)
if only_kessan and len(filtered) == 0:
    st.warning("æ±ºç®—çŸ­ä¿¡ï¼ˆå³å¯†ï¼‰ã§ã¯0ä»¶ã§ã—ãŸã€‚æ±ºç®—é–¢é€£ï¼ˆåºƒã‚ï¼‰ã«è‡ªå‹•åˆ‡æ›¿ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
    filtered = build_filtered(use_strict=False)

# ----------------------------
# AI availability
# ----------------------------
ai_ok = ai_is_enabled()
if show_ai_button and not ai_ok:
    st.warning("Gemini APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€AIåˆ†æã¯ç„¡åŠ¹ã§ã™ï¼ˆè¡¨ç¤ºã®ã¿ï¼‰ã€‚Secretsã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# Render list
# ----------------------------
st.subheader(f"å€™è£œï¼š{len(filtered)}ä»¶")
if not filtered:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹é–‹ç¤ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥æ•°/ä»¶æ•°/ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚¹ãƒãƒ›å‰æï¼š1ä»¶ãšã¤expander
for i, it in enumerate(filtered[:200]):  # è¡¨ç¤ºä¸Šé™ï¼ˆé‡ããªã‚‹ã®ã§ï¼‰
    title = it.get("title", "")
    code_disp = it.get("code_disp", "") or "----"
    doc_url = (it.get("doc_url") or "").strip()
    published = it.get("published_at")
    uid = it.get("uid") or hashlib.md5(f"{i}".encode()).hexdigest()[:12]

    published_str = published.astimezone(timezone.utc).strftime("%Y-%m-%d %H:%M UTC") if isinstance(published, datetime) else "ä¸æ˜"
    label = f"{code_disp}ï½œ{published_str}ï½œ{title or '(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)'}"

    with st.expander(label, expanded=False):
        if doc_url:
            st.caption(f"PDF: {doc_url}")
        else:
            st.caption("URLæƒ…å ±ãªã—ï¼ˆAIè§£æä¸å¯ï¼‰")

        cached = get_cached_analysis(DB_PATH, doc_url) if doc_url else None
        if cached:
            st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
            render_analysis(cached)
        else:
            st.info("æœªè§£æ")

        cols = st.columns([1, 1, 2])

        with cols[0]:
            if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¡¨ç¤º", key=f"show_{uid}") and cached:
                render_analysis(cached)

        with cols[1]:
            can_run_ai = show_ai_button and ai_ok and bool(doc_url)
            run = st.button("AIåˆ†æ", key=f"ai_{uid}", disabled=not can_run_ai)

        with cols[2]:
            st.caption("â€»åŒã˜PDF URLã¯SQLiteã«ä¿å­˜ã—ã€å†è§£æã—ã¾ã›ã‚“ï¼ˆDBã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ‰±ã„ï¼‰ã€‚")

        if run:
            with st.spinner("AIãŒæ±ºç®—è³‡æ–™ã‚’è§£æä¸­..."):
                try:
                    payload = analyze_pdf_to_json(doc_url)
                    save_analysis(
                        DB_PATH,
                        doc_url,
                        it.get("code", ""),
                        title,
                        published,
                        payload,
                    )
                    st.success("è§£æå®Œäº†")
                    render_analysis(payload)
                except Exception as e:
                    st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

st.divider()

# Manual analyze
st.subheader("æ‰‹å‹•è§£æï¼ˆPDF URLã‚’è²¼ã‚‹ï¼‰")
manual = st.text_input("PDF URLï¼ˆ.pdfæ¨å¥¨ï¼‰", value="").strip()
colA, colB = st.columns([1, 3])
with colA:
    manual_run = st.button("AIè§£æ", disabled=not (ai_ok and manual))
with colB:
    st.caption("â€»PDFä»¥å¤–ã®URLã ã¨å¤±æ•—ã—ã¾ã™ï¼ˆHTMLãªã©ï¼‰ã€‚")

if manual_run:
    with st.spinner("AIãŒè§£æä¸­..."):
        payload = analyze_pdf_to_json(manual)
    st.json(payload)
