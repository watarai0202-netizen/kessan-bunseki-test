import hashlib
import re
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional, Tuple

import streamlit as st

from src.tdnet import fetch_tdnet_items
from src.analyzer import analyze_pdf_to_json, ai_is_enabled
from src.storage import init_db, get_cached_analysis, save_analysis, db_path_default
from src.viz import render_analysis

# ----------------------------
# Helpers
# ----------------------------

# æ±ºç®—ã£ã½ã„ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®šï¼ˆã‚†ã‚‹ã‚ï¼‰
_KESSAN_RE = re.compile(
    r"(æ±ºç®—çŸ­ä¿¡|å››åŠæœŸæ±ºç®—|é€šæœŸæ±ºç®—|Financial Results|Earnings|Results)",
    re.IGNORECASE,
)

def is_kessan(title: str) -> bool:
    return bool(_KESSAN_RE.search(title or ""))


def _parse_dt_any(value: Any) -> Optional[datetime]:
    """
    published_at ã®æºã‚Œã«è€ãˆã‚‹ï¼š
      - ISO: 2026-02-06T20:00:00Z / +09:00
      - ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š: 2026-02-06 20:00:00  (â†JSTæƒ³å®š)
    è¿”ã‚Šå€¤ã¯UTC tz-aware datetime
    """
    if not value:
        return None
    s = str(value).strip()
    if not s:
        return None

    # ISO Z å¯¾å¿œ
    s_iso = s.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(s_iso)
        if dt.tzinfo is None:
            # tzç„¡ã—ã¯JSTæƒ³å®šï¼ˆTDnetã®pubdateãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šã„ï¼‰
            dt = dt.replace(tzinfo=timezone(timedelta(hours=9)))
        return dt.astimezone(timezone.utc)
    except Exception:
        pass

    # "YYYY-MM-DD HH:MM:SS"
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
        try:
            dt = datetime.strptime(s, fmt).replace(tzinfo=timezone(timedelta(hours=9)))
            return dt.astimezone(timezone.utc)
        except Exception:
            continue

    return None


def _extract_tdnet_fields(it: Dict[str, Any]) -> Tuple[str, str, str, Optional[datetime]]:
    """
    it ã®æ­£è¦åŒ–ãŒå£Šã‚Œã¦ã‚‚ app å´ã§å¾©å…ƒã™ã‚‹ï¼ˆå£Šã‚Œã¥ã‚‰ã•å„ªå…ˆï¼‰ã€‚
    æˆ»ã‚Š: (title, code, doc_url, published_at_utc)
    """
    title = (it.get("title") or "").strip()
    code = str(it.get("code") or "").strip()
    doc_url = (it.get("doc_url") or "").strip()
    published_at = it.get("published_at")

    # published_at ãŒ datetime ã§æ¥ã¦ãªã‘ã‚Œã°ãƒ‘ãƒ¼ã‚¹
    if not isinstance(published_at, datetime):
        published_at = _parse_dt_any(published_at)

    # raw ã‹ã‚‰æ•‘æ¸ˆï¼ˆit["raw"] ã®ä¸‹ãŒ Tdnet/TDnet/ç›´ä¸‹ ãªã©æºã‚Œã‚‹ï¼‰
    raw = it.get("raw") if isinstance(it.get("raw"), dict) else {}
    td = None
    if isinstance(raw.get("Tdnet"), dict):
        td = raw["Tdnet"]
    elif isinstance(raw.get("TDnet"), dict):
        td = raw["TDnet"]
    elif isinstance(raw.get("tdnet"), dict):
        td = raw["tdnet"]
    elif isinstance(raw, dict):
        td = raw

    if isinstance(td, dict):
        if not title:
            title = str(td.get("title") or td.get("Title") or "").strip()

        # 4æ¡/5æ¡æºã‚Œï¼šcompany_code ãŒ 45230 ã¿ãŸã„ã«æœ«å°¾0ã®ã“ã¨ãŒã‚ã‚‹
        if not code:
            code = str(td.get("code") or td.get("company_code") or td.get("Code") or "").strip()

        if not doc_url:
            doc_url = str(
                td.get("document_url")
                or td.get("documentUrl")
                or td.get("doc_url")
                or td.get("url")
                or ""
            ).strip()

        if published_at is None:
            published_at = _parse_dt_any(td.get("published_at") or td.get("pubdate") or td.get("date"))

    return title, code, doc_url, published_at


def _code4(code: str) -> str:
    """
    45230 -> 4523 ã¿ãŸã„ãªæ•‘æ¸ˆï¼ˆæœ«å°¾0ãŒä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ç”¨ï¼‰ã€‚
    ãŸã ã—å¿…ãšãã†ã¨ã¯é™ã‚‰ãªã„ã®ã§ã€è¡¨ç¤ºã¯ (å…ƒã‚³ãƒ¼ãƒ‰) ã‚‚ä½µè¨˜ã™ã‚‹ã€‚
    """
    c = (code or "").strip()
    if len(c) == 5 and c.isdigit() and c.endswith("0"):
        return c[:-1]
    if len(c) >= 4 and c[:4].isdigit():
        return c[:4]
    return c


def _is_allowed_pdf_url(url: str) -> bool:
    """
    æ‰‹å‹•URLè§£æã®å®‰å…¨ç­–ï¼ˆå£Šã‚Œé˜²æ­¢ï¼‰ã€‚
    - release.tdnet.info ã®PDF
    - yanoshin rd.php çµŒç”±ã§ release.tdnet.info ã®PDF
    """
    u = (url or "").strip()
    if not u:
        return False
    u_low = u.lower()
    if "release.tdnet.info" in u_low and u_low.endswith(".pdf"):
        return True
    if "webapi.yanoshin.jp/rd.php?" in u_low and "release.tdnet.info" in u_low and ".pdf" in u_low:
        return True
    return False


# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒŠãƒ¼", layout="wide")

# ----------------------------
# Auth (simple password gate)
# ----------------------------
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
if not APP_PASSWORD:
    st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆStreamlit Cloud ã® Secrets ã‹ã€ãƒ­ãƒ¼ã‚«ãƒ«ã® .streamlit/secrets.toml ã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
    pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pw and pw == APP_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
    st.stop()

# ----------------------------
# DB init (cache store)
# ----------------------------
DB_PATH = st.secrets.get("DB_PATH", db_path_default())
init_db(DB_PATH)

# ----------------------------
# Header
# ----------------------------
st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")
max_pdf_bytes = int(st.secrets.get("MAX_PDF_BYTES", 0) or 0)
if max_pdf_bytes > 0:
    st.caption(f"ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚ / PDFä¸Šé™: {max_pdf_bytes/1024/1024:.1f}MB")
else:
    st.caption("ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚")

# ----------------------------
# Screening controls
# ----------------------------
with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    col1, col2, col3 = st.columns([2, 2, 2])

    with col1:
        code_in = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã€ç©ºãªã‚‰ç›´è¿‘å…¨ä½“ï¼‰", value="").strip()
        only_kessan = st.checkbox("æ±ºç®—çŸ­ä¿¡ã ã‘ã«çµã‚‹ï¼ˆ0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«åˆ‡æ›¿ï¼‰", value=True)

    with col2:
        days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 30, 3)
        limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆå¤§ãã„ã»ã©é…ã„ï¼‰", 50, 500, 200)

    with col3:
        only_has_doc_url = st.checkbox("PDF URLãŒã‚ã‚‹ã‚‚ã®ã ã‘", value=False)
        show_ai_button = st.checkbox("AIåˆ†æãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º", value=True)
        show_debug = st.checkbox("DEBUGè¡¨ç¤ºï¼ˆå…ˆé ­5ä»¶ã®JSONï¼‰", value=False)

# sanity for code
code = ""
if code_in:
    if code_in.isdigit() and len(code_in) == 4:
        code = code_in
    else:
        st.warning("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¯4æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š7203ï¼‰")

# ----------------------------
# Fetch TDnet index (non-scrape)
# ----------------------------
cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)
with st.spinner("é–‹ç¤ºä¸€è¦§ã‚’å–å¾—ä¸­..."):
    items = fetch_tdnet_items(code or None, limit=limit)

if show_debug:
    st.subheader("DEBUG: items å…ˆé ­5ä»¶ï¼ˆtitle/code/doc_url/link ã®æºã‚Œç¢ºèªï¼‰")
    st.json(items[:5])

# ----------------------------
# Normalize + Filter
# ----------------------------
normalized: list[dict[str, Any]] = []
for it in items:
    if not isinstance(it, dict):
        continue
    title, code_raw, doc_url, published_at = _extract_tdnet_fields(it)
    code4 = _code4(code_raw)

    normalized.append(
        {
            "title": title,
            "code": code4,
            "code_raw": code_raw,
            "doc_url": doc_url,
            "published_at": published_at,  # UTC
            "raw": it.get("raw") if isinstance(it.get("raw"), dict) else it,
        }
    )

def apply_filters(use_kessan: bool) -> list[dict[str, Any]]:
    out: list[dict[str, Any]] = []
    for it in normalized:
        title = it.get("title", "")
        doc_url = (it.get("doc_url") or "").strip()
        published = it.get("published_at")

        if use_kessan and not is_kessan(title):
            continue
        if only_has_doc_url and not doc_url:
            continue
        if isinstance(published, datetime) and published < cutoff_utc:
            continue
        out.append(it)
    return out

filtered = apply_filters(only_kessan)

# 0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«ã™ã‚‹
if only_kessan and not filtered:
    st.info("ã€æ±ºç®—çŸ­ä¿¡ã ã‘ã€ã§0ä»¶ã ã£ãŸã®ã§ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’åºƒã’ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
    filtered = apply_filters(False)

st.subheader(f"å€™è£œï¼š{len(filtered)}ä»¶")
if not filtered:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹é–‹ç¤ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥æ•°ã‚„ä»¶æ•°ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# AI availability
ai_ok = ai_is_enabled()
if show_ai_button and not ai_ok:
    st.warning("Gemini APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€AIåˆ†æã¯ç„¡åŠ¹ã§ã™ï¼ˆæ•°å€¤è¡¨ç¤ºã®ã¿ï¼‰ã€‚Secretsã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# Render list
# ----------------------------
# ã‚¹ãƒãƒ›å‰æï¼š1ä»¶ãšã¤ expander ã§é–‹ã UI
for i, it in enumerate(filtered[:100]):
    title = it.get("title", "")
    code_ = it.get("code", "") or "----"
    code_raw = it.get("code_raw", "") or ""
    doc_url = (it.get("doc_url") or "").strip()
    published = it.get("published_at")

    # è¡¨ç¤ºç”¨æ—¥æ™‚ï¼ˆUTCâ†’JSTï¼‰
    if isinstance(published, datetime):
        published_jst = published.astimezone(timezone(timedelta(hours=9)))
        published_str = published_jst.strftime("%Y-%m-%d %H:%M JST")
    else:
        published_str = "æ—¥æ™‚ä¸æ˜"

    # ä¸€æ„ã‚­ãƒ¼ï¼ˆDuplicateElementKeyå¯¾ç­–ï¼‰
    seed = f"{code_}|{code_raw}|{published_str}|{title}|{doc_url}|{i}"
    uid = hashlib.md5(seed.encode("utf-8")).hexdigest()[:12]

    label = f"{code_}({code_raw})ï½œ{published_str}ï½œ{title}"
    with st.expander(label, expanded=False):
        if doc_url:
            st.caption(f"PDF: {doc_url}")
            st.link_button("PDFã‚’é–‹ã", doc_url)
        else:
            st.caption("PDF: ï¼ˆãªã—ï¼‰")

        cached = get_cached_analysis(DB_PATH, doc_url) if doc_url else None
        if cached:
            st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
            render_analysis(cached)
        else:
            st.info("æœªè§£æ")

        cols = st.columns([1, 1, 2])

        with cols[0]:
            if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¡¨ç¤º", key=f"show_{uid}") and cached:
                render_analysis(cached)

        with cols[1]:
            can_run_ai = show_ai_button and ai_ok and bool(doc_url)
            run = st.button("AIåˆ†æ", key=f"ai_{uid}", disabled=not can_run_ai)

        with cols[2]:
            st.caption("â€»åŒã˜PDF URLã¯SQLiteã«ä¿å­˜ã—ã€å†è§£æã—ã¾ã›ã‚“ï¼ˆDBã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ‰±ã„ï¼‰ã€‚")

        if run:
            with st.spinner("AIãŒæ±ºç®—çŸ­ä¿¡ã‚’è§£æä¸­..."):
                try:
                    payload = analyze_pdf_to_json(doc_url)
                    save_analysis(DB_PATH, doc_url, code_, title, published, payload)
                    st.success("è§£æå®Œäº†")
                    render_analysis(payload)
                except Exception as e:
                    st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

st.divider()

# ----------------------------
# Manual analyze
# ----------------------------
st.subheader("æ‰‹å‹•è§£æï¼ˆPDF URLã‚’è²¼ã‚‹ï¼‰")
manual = st.text_input("PDF URLï¼ˆrelease.tdnet.info ã® .pdf æ¨å¥¨ï¼‰", value="").strip()

colA, colB = st.columns([1, 3])
with colA:
    manual_ok = ai_ok and _is_allowed_pdf_url(manual)
    manual_run = st.button("AIè§£æ", disabled=not manual_ok)

with colB:
    if manual and not _is_allowed_pdf_url(manual):
        st.warning("å®‰å…¨ã®ãŸã‚ã€release.tdnet.info ã®PDFï¼ˆã¾ãŸã¯ yanoshin rd.php çµŒç”±ï¼‰ä»¥å¤–ã¯ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã¾ã™ã€‚")
    else:
        st.caption("â€»AIæœ‰åŠ¹ï¼‹è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã®PDF URLã®ã¿è§£æã—ã¾ã™ã€‚")

if manual_run:
    with st.spinner("AIãŒè§£æä¸­..."):
        try:
            payload = analyze_pdf_to_json(manual)
            st.success("è§£æå®Œäº†")
            render_analysis(payload)
        except Exception as e:
            st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
