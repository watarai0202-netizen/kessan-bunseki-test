import os
import json
import sqlite3
import tempfile
from datetime import datetime, timedelta, timezone

import requests
import streamlit as st
from google import genai  # pip install google-genai

# ----------------------------
# Settings / Secrets
# ----------------------------
API_KEY = st.secrets.get("GEMINI_API_KEY", "")
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")

if not API_KEY:
    st.error("GEMINI_API_KEY ãŒæœªè¨­å®šã§ã™ï¼ˆStreamlit Secretsã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

client = genai.Client(api_key=API_KEY)

DB_PATH = "app.db"

# ã‚„ã®ã—ã‚“ TDnet WEB-APIï¼ˆJSONãŒæ‰±ã„ã‚„ã™ã„ï¼‰
TDNET_BASE = "https://webapi.yanoshin.jp/webapi/tdnet/list"

# ----------------------------
# Auth
# ----------------------------
def require_login():
    if not APP_PASSWORD:
        st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆSecretsã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
        st.stop()

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if not st.session_state.authenticated:
        st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
        pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
        if pw == APP_PASSWORD:
            st.session_state.authenticated = True
            st.rerun()
        st.stop()

# ----------------------------
# Storage (SQLite)
# ----------------------------
def init_db():
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS analyses (
      doc_url TEXT PRIMARY KEY,
      code TEXT,
      title TEXT,
      published_at TEXT,
      payload_json TEXT,
      created_at TEXT
    )
    """)
    con.commit()
    con.close()

def get_cached_analysis(doc_url: str):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("SELECT payload_json FROM analyses WHERE doc_url=?", (doc_url,))
    row = cur.fetchone()
    con.close()
    if row:
        return json.loads(row[0])
    return None

def save_analysis(doc_url, code, title, published_at, payload: dict):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("""
      INSERT OR REPLACE INTO analyses(doc_url, code, title, published_at, payload_json, created_at)
      VALUES(?,?,?,?,?,?)
    """, (doc_url, code, title, published_at, json.dumps(payload, ensure_ascii=False), datetime.now().isoformat()))
    con.commit()
    con.close()

# ----------------------------
# TDnet Fetch
# ----------------------------
@st.cache_data(ttl=60, show_spinner=False)
def fetch_tdnet_list_json(code: str | None, days: int = 3, has_xbrl: bool = False, limit: int = 200):
    """
    æŒ‡å®šéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯recentã‹ã‚‰ç›´è¿‘ã‚’å–å¾—ã€‚
    daysã¯å–å¾—ã®â€œè¦–é‡â€ã§ã€ã‚¢ãƒ—ãƒªå´ã§ã•ã‚‰ã«ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹å‰æï¼ˆå£Šã‚Œã«ãã„ï¼‰ã€‚
    """
    if code and code.isdigit() and len(code) == 4:
        # éŠ˜æŸ„åˆ¥ï¼ˆAtom/RSSã‚‚ã‚ã‚‹ãŒJSONãŒæ‰±ã„ã‚„ã™ã„ï¼‰
        url = f"{TDNET_BASE}/{code}.json?limit={limit}"
    else:
        url = f"{TDNET_BASE}/recent.json?limit={limit}"

    if has_xbrl:
        url += "&hasXBRL=1"

    r = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
    r.raise_for_status()
    data = r.json()

    # dataã®å½¢ã¯APIå´ã®ä»•æ§˜ã«ä¾å­˜ã™ã‚‹ã®ã§ã€itemsã‚’â€œãã‚Œã£ã½ãâ€æ­£è¦åŒ–ã—ã¦è¿”ã™
    items = []
    for it in data.get("items", []):
        td = it.get("TDnet") or it  # json/json2å·®åˆ†å¯¾ç­–
        title = td.get("title", "")
        doc_url = td.get("document_url", "")  # hasXBRL=1ãªã‚‰XBRLã¸ã®ãƒªãƒ³ã‚¯ã«ãªã‚Šå¾—ã‚‹ç‚¹ã«æ³¨æ„ :contentReference[oaicite:10]{index=10}
        code_ = str(td.get("code", "")) if td.get("code") else ""
        published = td.get("published_at") or td.get("pubdate") or td.get("date") or ""

        items.append({
            "code": code_,
            "title": title,
            "doc_url": doc_url,
            "published": published,
        })

    # ç›´è¿‘daysæ—¥ä»¥å†…ã£ã½ã„ã‚‚ã®ã ã‘ï¼ˆpublishedãŒå–ã‚Œãªã„å ´åˆã¯æ®‹ã™ï¼‰
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    filtered = []
    for x in items:
        p = x["published"]
        if not p:
            filtered.append(x)
            continue
        try:
            dt = datetime.fromisoformat(p.replace("Z", "+00:00"))
            if dt >= cutoff:
                filtered.append(x)
        except Exception:
            filtered.append(x)

    return filtered

# ----------------------------
# PDF Download
# ----------------------------
def download_to_temp(url: str) -> str:
    r = requests.get(url, timeout=30, allow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
    r.raise_for_status()
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    tmp.write(r.content)
    tmp.close()
    return tmp.name

# ----------------------------
# Gemini JSON Extraction
# ----------------------------
def analyze_doc_with_gemini(doc_url: str) -> dict:
    """
    æ±ºç®—çŸ­ä¿¡PDFï¼ˆã¾ãŸã¯PDFãƒªãƒ³ã‚¯ï¼‰ã‚’èª­ã¿ã€å¯è¦–åŒ–ã§ãã‚‹JSONã‚’è¿”ã™ã€‚
    """
    cached = get_cached_analysis(doc_url)
    if cached:
        return cached

    pdf_path = None
    try:
        pdf_path = download_to_temp(doc_url)

        uploaded = client.files.upload(file=pdf_path)

        # â€œå¯è¦–åŒ–å¯èƒ½ãªJSONâ€ã«å›ºå®šã™ã‚‹ã®ãŒãƒã‚¤ãƒ³ãƒˆ
        prompt = """
ã‚ãªãŸã¯æ—¥æœ¬æ ªã®æ±ºç®—çŸ­ä¿¡ã‚’æŠ•è³‡å®¶ç›®ç·šã§åˆ†æã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
æ·»ä»˜PDFï¼ˆæ±ºç®—çŸ­ä¿¡ï¼‰ã‹ã‚‰ã€æ¬¡ã®JSONã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ç¦æ­¢ï¼‰ã€‚
æ•°å€¤ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ null ã«ã—ã¦ãã ã•ã„ã€‚å˜ä½ã¯å¯èƒ½ãªã‚‰ã€Œç™¾ä¸‡å††ã€ã€Œå††ã€ãªã©ã‚’æ˜è¨˜ã€‚

JSONã‚¹ã‚­ãƒ¼ãƒï¼ˆå³å®ˆï¼‰:
{
  "summary_1min": "string",
  "headline": {
    "tone": "å¼·æ°—|ä¸­ç«‹|å¼±æ°—|ä¸æ˜",
    "score_0_10": number
  },
  "performance": {
    "period": "ä¾‹: 2025å¹´åº¦3Q ãªã©",
    "sales_yoy_pct": number|null,
    "op_yoy_pct": number|null,
    "ordinary_yoy_pct": number|null,
    "net_yoy_pct": number|null
  },
  "guidance": {
    "raised": true|false|null,
    "lowered": true|false|null,
    "unchanged": true|false|null,
    "sales_full_year": number|null,
    "op_full_year": number|null,
    "eps_full_year": number|null
  },
  "drivers": {
    "profit_up_reasons": ["string", "..."],
    "profit_down_reasons": ["string", "..."]
  },
  "risks": {
    "short_term": ["string", "..."],
    "mid_term": ["string", "..."]
  },
  "watch_points": ["string", "..."]
}
"""

        resp = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[prompt, uploaded],
            # ã“ã“ã¯SDKå´ã®æŒ™å‹•å·®ãŒã‚ã‚‹ã®ã§ã€å£Šã‚Œã«ããã™ã‚‹ãŸã‚ã€ŒJSONã‚’è¿”ã›ã€ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç¸›ã£ã¦ãŠã
        )

        text = (resp.text or "").strip()

        # æœ€ä½é™ã®JSONãƒ‘ãƒ¼ã‚¹ï¼ˆå£Šã‚ŒãŸã‚‰ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼‰
        payload = json.loads(text)
        return payload

    finally:
        if pdf_path and os.path.exists(pdf_path):
            try:
                os.remove(pdf_path)
            except Exception:
                pass

# ----------------------------
# UI
# ----------------------------
init_db()
require_login()

st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")

with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    code = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã€ç©ºãªã‚‰å…¨ä½“ã®ç›´è¿‘ï¼‰", value="")
    days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 14, 3)
    only_xbrl = st.checkbox("XBRLãŒã‚ã‚‹é–‹ç¤ºã ã‘ï¼ˆè¦‹ã¤ã‹ã‚‹ç‡â†‘ï¼‰", value=False)
    limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆé‡ã„ã»ã©é…ã„ï¼‰", 50, 300, 200)

items = fetch_tdnet_list_json(code.strip() or None, days=days, has_xbrl=only_xbrl, limit=limit)

# æ±ºç®—çŸ­ä¿¡ã£ã½ã„ã‚‚ã®ã ã‘
kessan = [x for x in items if "æ±ºç®—çŸ­ä¿¡" in (x["title"] or "")]
st.subheader(f"å€™è£œï¼š{len(kessan)}ä»¶ï¼ˆæ±ºç®—çŸ­ä¿¡ã®ã¿ï¼‰")

if not kessan:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æ±ºç®—çŸ­ä¿¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()

for x in kessan[:50]:
    title = x["title"]
    doc_url = x["doc_url"]
    code_ = x["code"]
    published = x["published"]

    with st.container(border=True):
        st.write(f"**{code_}**  {title}")
        if published:
            st.caption(f"å…¬é–‹: {published}")
        st.caption(doc_url)

        colA, colB = st.columns([1, 2])
        with colA:
            run = st.button("åˆ†æã—ã¦è¡¨ç¤º", key=doc_url)
        with colB:
            st.caption("â€»åŒã˜URLã¯DBã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ï¼ˆå†è§£æã—ã¾ã›ã‚“ï¼‰")

        if run:
            with st.spinner("è§£æä¸­ï¼ˆåˆå›ã¯æ•°åç§’ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼‰"):
                payload = analyze_doc_with_gemini(doc_url)

            # ä¿å­˜
            save_analysis(doc_url, code_, title, published, payload)

            # å¯è¦–åŒ–ï¼ˆæœ€ä½é™ï¼‰
            st.markdown("### 1åˆ†è¦ç´„")
            st.write(payload.get("summary_1min", ""))

            st.markdown("### ãƒˆãƒ¼ãƒ³ / ã‚¹ã‚³ã‚¢")
            headline = payload.get("headline", {})
            st.write(f"ãƒˆãƒ¼ãƒ³: {headline.get('tone')} / ã‚¹ã‚³ã‚¢: {headline.get('score_0_10')}")

            st.markdown("### å‰å¹´æ¯”ï¼ˆ%ï¼‰")
            perf = payload.get("performance", {})
            chart_data = {
                "sales_yoy_pct": perf.get("sales_yoy_pct"),
                "op_yoy_pct": perf.get("op_yoy_pct"),
                "ordinary_yoy_pct": perf.get("ordinary_yoy_pct"),
                "net_yoy_pct": perf.get("net_yoy_pct"),
            }
            # æ•°å€¤ã ã‘æŠ½å‡ºã—ã¦æ£’ã‚°ãƒ©ãƒ•
            numeric = {k: v for k, v in chart_data.items() if isinstance(v, (int, float))}
            if numeric:
                st.bar_chart(numeric)
            else:
                st.info("å‰å¹´æ¯”ã®æ•°å€¤ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFã®æ›¸å¼å·®ã®å¯èƒ½æ€§ï¼‰ã€‚")

            st.markdown("### å¢—æ¸›ç›Šç†ç”± / ãƒªã‚¹ã‚¯")
            drivers = payload.get("drivers", {})
            st.write("å¢—ç›Šç†ç”±:", drivers.get("profit_up_reasons", []))
            st.write("æ¸›ç›Šç†ç”±:", drivers.get("profit_down_reasons", []))

            risks = payload.get("risks", {})
            st.write("çŸ­æœŸãƒªã‚¹ã‚¯:", risks.get("short_term", []))
            st.write("ä¸­æœŸãƒªã‚¹ã‚¯:", risks.get("mid_term", []))

st.divider()
st.subheader("æ‰‹å‹•ï¼ˆPDF URLã‚’è²¼ã£ã¦è§£æï¼‰")
manual = st.text_input("æ±ºç®—çŸ­ä¿¡PDFã®URL")
if st.button("æ‰‹å‹•è§£æ") and manual:
    with st.spinner("è§£æä¸­..."):
        payload = analyze_doc_with_gemini(manual)
    st.json(payload)
